<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8" />
  <title>lpil - Build Your Own Elixir</title>
  <link rel="stylesheet" href="style.css" type="text/css" charset="utf-8">
</head>
  <body>
    <textarea id="source">
class: center, middle, inverse

# Build Your Own Elixir

Louis Pilfold<br>
<a href="http://lpil.uk/" target="_blank">http://lpil.uk/</a><br>
<a href="https://twitter.com/louispilfold" target="_blank">@louispilfold</a>


???

Hi. I'm Louis Pilfold.

I started writing Elixir and Erlang a couple years ago and through various pet
projects I went from knowing nothing about compilers to somehow having a little
language on the BEAM. I learnt about tokenization and parsing, abstract syntax
trees and code generation, and what this can look like with Elixir and Erlang.
I'd like to share this journey with you, and also share some of the things I've
learnt along the way.

---
class: center, middle

![Rubocop](assets/rubocop.png)

???

When I discovered Elixir I was professionally writing Ruby. Back in the Ruby
world I had developed a soft spot for static analysis programs. One such tool
was the style linter Rubocop, which is a program that inspects your codebase
for style errors and common mistakes. I find they're handy for keeping a
codebase consistent, and also for avoiding arguments about indentation.

Being a young language Elixir didn't an equivilent tool, so I decided to
take a shot at making one myself.

---
class: middle, large

```plain
Source code
  -> Errors
```

???

Linters are effectvely functions that take source code files as an
input, and return a number of errors to the user. I didn't know exactly how
they worked so I looked at source for Rubocop and a Javascript linter called
JSCS.

I discovered that if you look a little closer they look like something like
this.

---
class: middle, large

```plain
Source code
  -> Data structures
  -> Errors
```
???

They take source code, convert them to one or more different data structures,
and then analyse those forms to find any errors.

These errors would show up as patterns in the data that we could match against.

---
class: middle, large

```plain
Source code
  -> Tokens
  -> Errors
```
???

The first data structure we can create from source code is a list of tokens.

Tokens represents the smallest elemental parts of the source code we type, the
basic textual building blocks of code.

---
class: middle, large
```ruby
"Hello, world!"

- word "Hello"
- comma
- space
- word "world"
- exclaimation mark
```
???

For example, if we took the English sentence "Hello, world!" and tokenize it
we might end up with something like this.

There are have 5 tokens

- The first one is a word with a value of "Hello".
- The second is a comma.
- The third is a space.
- The fourth is word with a value of "world".
- And lastly we an exclaimation mark.

---
class: middle, large
```ruby
1 |> add 2

- number 1
- arrow_op |>
- identifier add
- number 2
```
???

Here we can do the same with Elixir code.

Here's a snippet of code in which I pipe the number one into a function called
"add". The function also takes an additional argument of 2. When tokenized it
becomes this a list of 4 tokens:

- A number token with a value of 1.
- An arrow_op token which is the pipe operator.
- An identifier token of the value add.
- And another number token with the value of 2.

---
class: middle, large
```ruby
1 |> add 2

[{:number, _, 1},
 {:arrow_op, _, :|>},
 {:identifier, _, :add},
 {:number, _, 2}]
```
???

This data in Elixir terms would look like this. Each token is a tuple where the
first element is the name of the token type as an atom, and the last element
is the value of the token. The middle element contains some metadata, which
I've omitted.

In the Ruby and Javascript linters I looked at the tokenization process was
complex as the tokenizer had to be written from scratch. Luckily this wasn't
the case in Elixir.

---
class: middle, large
```ruby
:elixir_tokenizer.tokenize(
  '1 |> add 2 ', [], [])

{:ok, [], 12,
 [{:number, {[], 1, 2}, 1},
  {:arrow_op, {[], 3, 5}, :|>},
  {:identifier, {[], 6, 9}, :add},
  {:number, {[], 10, 11}, 2}]}
```
???

The Elixir compiler is written in Erlang, and the modules are available in
the Elixir standard library. Elixir tokenization is as simple as calling the
`tokenize` function from the `elixir_tokenizer` module.

So getting the tokens is easy. So how might they be used them in a linter?

---
class: middle, large
```ruby
IO.puts("Hello"); # Bad
IO.puts("World")  # Good
```
???

One simple thing an Elixir linter might do is forbid the use of semicolons to
separate expressions. Each expression should instead be seperated by newlines,
which is more idiomatic.

---
class: middle, large
```ruby
def semicolon?({:";", _, _}),
  do: true
def semicolon?(_),
  do: false

if Enum.any?(tokens, &semicolon?) do
  :error
else
  :ok
end
```
???

To make the linter detect violations of this rule I first defined a function
called "semicolons?". It returns true if passed a semicolon token, and false
if it is passed anything else. It detects the semicolon token by pattern
matching on the first element of the token tuple, which in this case is the
semicolon character as an atom.

Now with this function I can use the `Enum.any?` function to iterate over a
given list of tokens, and return an error if any semicolon tokens are found.

---
class: middle, large
```plain
Source code
  -> Tokens, Abstract Syntax Tree
  -> Errors
```
???

Tokens are simple and easy, but quite limited. In order to do more meaningful
analysis on code another form needs to be used to represent the source code-
this form is an abstract syntax tree.

---
class: middle, large
```ruby
# Code
add 1, 2
```
```ruby
# AST
function_call add
  ├─ number 1
  └─ number 2
```
???

While tokens were the linear sequence of all the elemental components of source
code, an abstract syntax tree is a representation of the the syntactic
structure of the source code.

Here's an example. On the first line is some code in which function
"add" is called with the arguments 1 and 2. Below that is the tree this
expression would form.

- The root node is a call to the function "add".
- This call node has 2 leaf node children.
- The first is the number 1, the second is the number 2.

---
class: middle, large
```ruby
send self(), {:compare, 2 + 2, 1}
```
```ruby
function_call send
  ├─ function_call self
  └─ tuple
     ├─ atom compare
     ├─ function_call +
     │  ├─ number 2
     │  └─ number 2
     └─ number 1
```
???

Here's a more complex example.

At the root of the tree there's a call to the "send" function, which has 2
arguments, and thus 2 children.

The first is a call to the zero arity function "self", and the second is a tuple.

The tuple has 3 elements, thus 3 children.

They are the atom "compare", a function call, and the number 1.

The final function call node is to the plus operator, and has 2 children, each
the number 2.

---
class: middle, large
```ruby
quote do
  add 1, 2
end
```
```ruby
{:add, [], [1, 2]}
```
???

Normally one would get this tree by first tokenizing the code, and then
parsing the tokens to construct the tree. In Elixir there is an easier method,
thanks to the macro system. When the `quote` special form is called on an
expression it returns an Elixir AST for the user to inspect or manipulate.

Elixir's AST is consise and simple. Everything that is not a literal in the
AST is a three item tuple where the first element is the name of the function
or constructor, the second item is some metadata, and the third item is a list
of that node's children.

Here is the add function again, only this time the `quote` special form is
being used to get real Elixir AST.

The root is a function call, so it's a three item tuple. The first element is
the atom "add" as that is the name of the function. The third element is the
node's children, which are the numbers 1 and 2.
---
class: middle, large
```ruby
# Good
if true do 2 else 1 end
```
```ruby
# Bad
unless true do 1 else 2 end
```
???

Now I've learnt a little about the AST and how to obtain it I can use it in
the linter. Say I want to forbid use of the `unless` macro with an `else`
block as I think it should be written with the `if` macro, to prevent the
hard-to-read double negative.

---
class: middle, large
```ruby
quote do
  unless true do
    1
  else
    2
  end
end
```
```ruby
{:unless, [], [true, [do: 1, else: 2]]}
```
???

With an AST I can enforce this by walking the tree until I come across the
offending pattern.

The offending pattern is a node with the atom `unless` in the first position,
making it a call to the unless macro, and an `else` block as the second child
of that node.

---
class: middle
```ruby
def check_unless(
  {:unless, _, [_, [do: _, else: _]]},
  status
),
  do: {node, :error}

def check_unless(node, status),
  do: {node, status}

Macro.prewalk(
  ast, :ok, &check_unless/2)
```
???

Traversing the AST is easy thanks to the `Macro.preawalk` function, which
takes an AST, an accumulator, and a callback that will receive each node. My
`check_unless/2` callback has two clauses. The first one pattern matches
against offending nodes and returns the atom `:error` in place of the
accumulator.

The other clause is a catch all for all other nodes. As all other patterns are
considered valid by this rule it just returns the accumulator.
---
class: middle, large

Tokens and ASTs == source code as data

`:elixir_tokenizer` for getting Elixir tokens

`quote/2` for getting Elixir AST

???

And with that I had the beginnings of a working linter, and I had also learnt
about tokens, abstract syntax trees and a little about parsing in general.

Through the macro system and associated helper functions Elixir had made this
task easier than it would be elsewhere.

All that was left was to do some plumbing to present errors to the user, and
then to write more rules. After writing a few more rules and then
experiementing with writing some macros I felt comfortable working with an
abstract syntax tree, giving me the first piece of the compiler knowledge
puzzle.

---
class: middle
```html
<!DOCTYPE html>
<html>
  <head>
    <title>Build Your Own Elixir</title>
  </head>
  <body>
    <h1 id="conf">
      An Elixir LDN talk
    </h1>
  </body>
</html>
```
???

Some time later I found myself making a simple web app in Elixir. Nothing
exciting, it rendered a few HTML pages to a user and let them record some
information in a database using forms.

While I was writing the HTML views I found myself getting a little fed up of
Elixir default templating language for making web pages. EEx is fast and easy
to use, but with it I still have to write regular HTML, and HTML is not so much
fun.

It has superfluous angle brackets, a rather verbose syntax for closing tags,
and you have to manually escape certain characters. I would rather type
something more concise instead, and when working with Ruby and Javascript I
could.

---
class: middle
```plain
html
  head
    title Build Your Own Elixir
  body
    h1#conf An Elixir LDN talk
```
???

There's a templating system for Ruby called Slim and another for Javascript
called Pug which allow me to write HTML like this. All the superfluous syntax
is gone, and the delimeters have been replaced with indentation. Granted, this
isn't everyone's cup of tea, but I've become accustomed to it, and as a result
I found myself wanting something similar in Elixir. Armed with my new-found
knowledge of tokenization and parsing I decided to make a similar library for
Elixir.

---
class: middle
```plain
HTML Template
  -> (data -> HTML String)
```
???

The templating library is effectvely a function that takes a template of
alternative HTML syntax, and returns a function that given data produces a
string of HTML.

---
class: middle
```plain
HTML Template
  -> Tokens
  -> AST
  -> (data -> HTML String)
```
???

In order to know what HTML to generate from the lightweight syntax I need to
yet again do some analysis on the source code, so like with the linter I need
to generate an AST, which I can inspect and do things with.

Unlike with the linter I don't have a pre-built function for getting tokens
from my html templates, so I'll have to build my own. After a little digging I
discovered that the Erlang standard library includes a module called Leex
which offers a DSL for creating tokenizers. It's the module that the LFE
language uses for tokenization, which is a pretty good endorsement in my
books.

It might be a little over the top as I could probably easily parse this string
manually with pattern matching, but this is a excuse to learn something new
while doing something useful, so lets get started.

---
class: middle
```plain
html
  head
    title Build Your Own Elixir
  body
    h1(id="conf") Elixir LDN 2016!
```
???

One line is one element in my syntax, so I'll split on newlines and trim the
intendation, leaving me with just the element syntax that I want to parse.

Looking at these elements I can see a few token types.

---
class: middle, large
```plain
h1#title.bold
h1    : name
#     : hash
title : name
.     : dot
bold  : name
```
???

There is one for names, which are element names, class names, or ID names,

There are dots and hashes which are used to denote classes and IDs
respectively.

---
class: middle, large
```plain
a(href="/")
```
```plain
a    : name
(    : open_paren
href : name
=    : equal
"/"  : string
)    : close_paren
```
???

The syntax for attributes includes open paren tokens, close paren tokens,
and then an equals token between the attribute name and the value.

There are strings, which is a series of characters surrounded by double
quotes.

---
class: middle, large
```plain
div ! Hi
```
```plain
div  : name
     : whitespace
!    : word
     : whitespace
Hi   : name
```
???

Lastly there's whitespace tokens, and word tokens, which are any
non-whitespace characters that are not covered by the other tokens.

Now I need to teach Leex what my tokens are so it can create the tokenizer.

---
class: middle
```plain
%%% my_tokenizer.xrl

Definitions.

% Token patterns

Rules.

% Mappings of patterns into token structures

Erlang code.

% Erlang helper functions here...
```
???

A Leex module is file that contains Erlang-like code and has the file
extension `.xrl`. Within it it has three sections:

"Definitions" in which the author uses regular expressions to define a pattern
for each type of token.

The "Rules" section in which the author declares what data structure should be
used to represent each token.

And lastly the "Erlang code" section which contains any helper functions that
might be used in the other sections.

---
class: middle
```plain
Definitions.

Dot    = \.
Hash   = #
EQ     = =
OpenP  = \(
CloseP = \)
String = "([^\\"]|\\.)*"
Name   = [A-Za-z][A-Za-z0-9_-]*
WS     = [\s\t]+
Word   = [^\(\)\t\s\n\.#=]+
```
???

Here is my Leex "Definitions" section, containing all my various token
patterns. Names are capitalized and go on the left hand side of the match
operator, patterns go on the right.

It has the simple literal patterns of Dot, Hash, EQ, OpenP and CloseP.

After that are the more complex patterns of String, Name, whitespace, and
Word. Regular expressions are not the easiest to read, let's go over them now.

---
class: middle, large
```plain
String = "([^\\"]|\\.)*"
Name   = [A-Za-z][A-Za-z0-9_-]*
WS     = [\s\t]+
Word   = [^\(\)\t\s\n\.#=]+
```
???

The string pattern is a pair of double quotes with zero or more characters
between them, where the characters are any non-double quote character, or any
character preceeded by an escaping slash. This is how we get support for
escaped quotes inside string bodies.

A name is any letter, followed by any mix of letters, numbers, underscores and
dashs.

Whitespace is one or more spaces and tabs.

and lastly a word is one or more or anything else.

---
class: middle
```plain
Rules.

{String} : {token, {string, TokenChars}}.
{Name}   : {token, {name,   TokenChars}}.
{Word}   : {token, {word,   TokenChars}}.
{Hash}   : {token, {hash,   TokenChars}}.
{Dot}    : {token, {dot,    TokenChars}}.
{EQ}     : {token, {eq,     TokenChars}}.
{WS}     : {token, {ws,     TokenChars}}.
{OpenP}  : {token, {'(',    TokenChars}}.
{CloseP} : {token, {')',    TokenChars}}.
```
???

After the "Definitions" section comes the "Rules" section, which is the
mapping between a pattern definition and a token data structure. The syntax for
a rule is the name of a definition in curly braces on the left, an instruction
tuple on the right, and a colon in the middle. Each rule ends with a full
stop, like in regular Erlang.

The first element in the tuple is the atom "token", which is an instruction to
output a token when this definition matches. The second item is the data
structure to be formed for this token. Here I'm always forming 2 item tuples
with the token name as an atom in the first position, and the matched
characters in the second position, which I access through the magic variable
"TokenChars".

Some of these definitions overlap and will match the same text, so in order
for the tokenizer to always produce the same results the order in which the
rules are checked is important. With Leex rules are checked from top to
bottom, and the first one that matches is the one that is used, much like a
case expression.

---
class: middle
```ruby
:my_tokenizer.string('div Hello world')
```
```ruby
{:ok, [
  name: 'div',
  ws:   ' ',
  name: 'Hello',
  ws:   ' ',
  name: 'world',
], _}
```
???

If I place this file in the `src` directory of an Elixir project Mix will
compile this to an Erlang module which exposes a `string/1` function. This
function takes a charlist of code and returns a list of tokens. Because I used
two item tuples with an atom as the first element for my tokens I get back an
Elixir keyword list.

Here I tokenize this line of code, and back I get

- a name token with a value of "div"
- a whitespace token
- a name token with a value of "Hello"
- a whitespace token
- and a name token with a value of "world". Great.

---
class: middle
```ruby
:my_tokenizer.string('a(href="/about")')
```
```ruby
{:ok, [
  name:   'a',
  "(":    '(',
  word:   'href',
  eq:     '=',
* string: '"/about"',
  ")":    ')',
], _}
```
???

At first this seemed enough, but when tokenizing another line I discovered a
problem. When I receive a string token the value is the string as written in
the source code, when I actually want the value of the string.

To resolve this I make use of the final part of a Leex module.

---
class: middle
```plain
{String} : {token, {string, string(TokenChars)}}.
% ...snip...

Erlang code.

strValue(S) ->
  tl(lists:droplast(S)).
```
???

Here I have updated token tuple for the string token to call a function called
string on the TokenChars before inserting it into the tuple.

The definition of this helper function goes in the "Erlang code" section. It
simply gets rid of the quotes by dropping the first and last characters with
the droplast and tail functions.

---
class: middle
```ruby
:my_tokenizer.string('a(href="/about")')
```
```ruby
{:ok, [
  name:   'a',
  "(":    '(',
  word:   'href',
  eq:     '=',
* string: '/about',
  ")":    ')',
], _}
```
???

Now I get the value I want for string tokens. Later I'll probably also want to
add helper functions for parsing numbers, handling escaped characters in
strings, and so on.

Right. With tokenizer done I can move onto building an AST from the token. In
the same way that Erlang supplies a tool for tokenization it also supplies a
tool for parsing, the Yecc module. Like Leex it's used by writing a module
with a specific syntax and file extension, which it then compiles into an
Erlang module.

This module contains a grammar, which is a set of rules that describe the
syntax of a language.


---
class: middle
```plain
%%% my_parser.yrl

Nonterminals.

Terminals.

Rootsymbol.

%% Grammar rules here...

Erlang code.
```
???

The file consists of 5 main sections. Nonterminals, Terminals, Rootsymbol,
grammar rules, and another Erlang code section for helper functions.

Terminals are the the most basic symbols recognised by the grammar, they
cannot be broken down into smaller parts. In this case these are all the
token types my Leex tokenizer can create.


---
class: middle
```plain
%%% my_parser.yrl

Nonterminals.

Terminals
'(' ')' name word dot hash string eq ws.

Rootsymbol.

%% Grammar rules here...

Erlang code.
```
???

Nonterminals are higher level symbols that are formed by composing
terminals, nonterminals or a mix both.

The Rootsymbol is the highest level nonterminal symbol that composes all the
others.

And lastly grammar rules are definitions about what symbols compose other
symbols, and in what context.

Lets take a look at Nonterminals.

---
class: middle
```pug
h1.jumbo
```
```ruby
terminal name: 'h1'
terminal dot:  '.'
terminal name: 'jumbo'
```
```ruby
terminal    name:  'h1'
nonterminal class: 'jumbo'
```
???

An example nonterminal in my grammar would be a class literal.

In the first code example is a h1 element with the class of "jumbo".

It tokenizes to three terminals, the name "h1", a dot, and the name "jumbo".

In this context the "dot" terminal followed by the "name" terminal can be
composed together to form the "class" nonterminal.

---
class: middle, large
```plain
class -> dot name    % .btn
id    -> hash name   % #jumbo
```
???

A class is a dot then a name.
An id is a hash then a name.

---
class: middle, large
```plain
classes -> class           % .big
classes -> class classes   % .small.tiny
```
???

Elements can have many class literals on them, so we need a classes
nonterminal.

classes are either a single class, or many classes. A repeating symbol like
this is defined recursively, so here classes can be a class, or a class
followed by classes.

---
class: middle, large
```plain
names -> name             % div
names -> name id          % div#header
names -> name classes     % div.btn
names -> name id classes  % div#send.btn
names -> classes          % .grey.small
names -> id               % #jumbo
names -> id classes       % #jumbo.border
```
???

Names is the head of an element in the HTML shorthand syntax.

- It can be a name
- A name then an id
- A name then classes
- A name then an id then classes
- Just classes
- Just an id
- Or an id then classes.

These declaritive rules continue. There's rules defining an attribute, many
attributes, pieces of text, and content that is composed of many pieces of
text and whitespace, until finally the Rootsymbol is reached. My Rootsymbol is
an element.

---
class: middle
```plain
% a.btn
element -> names

% a.btn(href="/about")
element -> names attributes

% a.btn(href="/about") About
element -> names attributes content

% a.btn About
element -> names content
```
???

- An element can be just a set of names,
- or it can be a set of names followed by attributes,
- or it can be names, then attributes, then content,
- or it can be just names followed by content.

And now that there are definitions for all the different symbols, from the
lowest terminals to the rootsymbol nonterminal.

With this Yecc has enough information to parse an element from a set of
tokens. The only thing left to do before it is capable of generating an
abstract syntax tree is instructing it how to build a data structure for each
nonterminal- what each node in the tree actually looks like.

---
class: middle, large
```plain
class -> dot name
```
- '$1' is the dot
- '$2' is the name
???

In my mini AST I want the class token to be represented by a string with the
same value as the token.

To achieve this I need to be able to refer to the tuple that makes up the name
token, and then extract string value from it.

Helpfully Yecc assigns pseudo variables in the form of atoms for each symbol
used in the nonterminal definition. If class is a dot then a name, atom dollar
one refers to the dot token, and atom dollar two refers to the name token.

---
class: middle, large
```plain
% .btn
'$1' = {dot,  "."}
'$2' = {name, "btn"}
```
```plain
class -> dot name : element(2, '$2').
```
???

The string I want is the second element in the tuple, so I can call the
`element` function on dollar two to get it. This code is placed after a colon
and before a full stop for each definition.

---
class: middle
```plain
class -> dot  name : element(2, '$2').
id    -> hash name : element(2, '$2').

classes -> class         : ['$1'].
classes -> class classes : ['$1' | '$2'].
```
???
<!-- this section is hard to read. Maybe just say that you form a list
     for collections.
  -->
Now that I've defined a data structure for class I can do the same for ID. It
also just pulls the string value from the name token.

Some nodes in my AST will be collections represented with a list. One such
example is the "classes" symbol, which is one or many class symbols.

For the base case of just one class I wrap the class in a list.

For the case of a class followed by classes we prepend the value of the class
to the value of classes, which unfolds recursively until we only have one
class, which is the base case that was just defined.

---
class: middle
```plain
h1#title.bold
```
```plain
% records.hrl
-record(
  names,
  { type    = "div"
  , id      = nil
  , classes = []
  }
).
```
???

Other symbols are more complex than simple values or lists. The "names" symbol
consist of a combination of

- an element `type` such as "div" or "body",
- an id,
- and one or more classes.

This could be represented as a three item tuple,
but then it's really hard to remember which field is which with tuples, so
instead I've opted to use an Erlang record.

Like Elixir structs each field in a record definition gets a name and default
value.

- The default for the type field is string "div",
- the default for the id field is the atom "nil",
- and the default for the classes field is an empty list.


---
class: middle, small
```plain
names -> classes      : #names{ classes = '$1' }.
names -> name classes : #names{ classes = '$2', type = '$1' }.
names -> id   classes : #names{ classes = '$2', id   = '$1' }.
names -> id           : #names{ id = '$1' }.
names -> name         : #names{ type = element(2, '$1') }.
names -> name id      : #names{ type = element(2, '$1'), id = '$2' }.
names -> name id classes
  : #names{ type = element(2, '$1') , class = '$3' , id = '$2' }.
```
???

With this record I can use a nice syntax for setting and getting named values
on a complex node.

Now that I can build simple values, more complex values, and collections
of values I can work my way all the way up to the rootsymbol, which the
element.

---
class: middle
```pug
a.profile(href="/me") User profile
```
```plain
element -> names attributeList content :
  #element{
    type       = '$1'#names.type,
    class      = '$1'#names.class,
    id         = '$1'#names.id,
    attributes = '$2',
    content    = '$3'
  }.
% ... other element definition clauses...
```
???

An Element is a record with a type, an id, some classes, attributes, and
content. For each definition I use the dollar variables to set the various
fields from the child nodes.

And with that all the definitions are in place, and I can place this file into
the `src` directory so that Mix can compile it into an Erlang module for me.

I can now turn source code into tokens, and tokens into an AST. The next step
is turning the AST into a function that produces HTML.

I wasn't sure how to do this, so I had a look at the source code for EEx on
GitHub. The code I found leveraged some of Elixir's metaprogramming features in
a way that to me seemed really clever and also easy to imitate. Yet Elixir was
doing all the hard work for me.

---
class: middle, large
```erb
Hello, <%= name %>
```
```ruby
def render(name) do
  "" &lt;&gt; "Hello, " &lt;&gt; name &lt;&gt; "!"
end
```
???

Here's an EEx template and the function that the EEx compiler would construct
for this template. The function simply concatenates each part of the template
together. Let's look at how this is done.


---
class: middle
```eex
Hello, <%= name %>
```
```ruby
[ text: "Hello, ",
  expr: " name " ]
```
???

EEx's parser splits the template into text and expressions. The template
in the first code snippet would be split into

- the text "Hello, ",
- and an expression consisting of the variable name.

This list is then turned into an expression which can be the body of a
function.

---
class: middle
```ruby
def compile(list) do
  Enum.reduce(list, "", &codegen/2)
end

def codegen({:text, text}, buffer) do
  quote do
    unquote(buffer) &lt;&gt; unquote(text)
  end
end
def codegen({:expr, text}, buffer) do
  ast = Code.string_to_quoted!(text)
  quote do
    unquote(buffer) &lt;&gt; unquote(ast)
  end
end
```
???

Here is the compile function, which constructs the expression. It reduces the
list with the codegen function, and uses an empty string as the starting value.

The codegen function has two clauses. The first is for text elements, which it
concatenates onto the buffer. Doing it inside the quote block like this
results in an AST being returned rather than the expression being evaluated.

The other clause is for expressions. It works exactly the same way as the
previous clause, except it calls `Code.string_to_quoted!` on the value first
in order to transform it from a string of Elixir code into an expression to be
injected into the quote block.

---
class: middle
```ruby
compile.([text: "Hello ", expr: "name", text: "!"])


# {:&lt;&gt;, [...],
#  [{:&lt;&gt;, [...],
#    [{:&lt;&gt;, [...], ["", "Hello "]},
#     {:name, [...], nil}]}, "!"]}
```
```ruby
compile.([text: "Hello ", expr: "name", text: "!"])
|> Macro.to_string
# (("" &lt;&gt; "Hello ") &lt;&gt; name) &lt;&gt; "!"
```
???

Here the compile function is called on the list from before.

It gets pretty hard to read these expressions, so at the bottom I'm
converting back into a string with the `Macro.to_string` function.

---
class: middle
```ruby
def render(name) do
  "" &lt;&gt; "Hello, " &lt;&gt; name &lt;&gt; "!"
end
```
```ruby
(("" &lt;&gt; "Hello ") &lt;&gt; name) &lt;&gt; "!"
```
???

On the top is the function from before, on the bottom is the string that was
just returned. They're pretty much identical.

So how would this work for the HTML templating library?

---
class: middle
```plain
h1#title = name
```
```ruby
%Element{ type: "h1", id: "title", classes: [],
  attributes: [], content: "= name", }
```
```ruby
[ text: "<h1 id='title'>",
  expr: " name",
  text: "</h1>" ]
```
```ruby
(("" &lt;&gt; "<h1 id='title'>") &lt;&gt; name) &lt;&gt; "</h1>"
```
???

The parser is used to generate an Elixir data structure with all the required
information from the template.
From the data structure a list of fragments of HTML text and expressions is
formed.

This list is put through the same compile function, which results in an Elixir
AST that builds the HTML string with the passed values injected into it.

All that's left is turning it into a function.

---
class: middle
```ruby
defmodule View do
  @ast """
  h1#title = name
  """
  |> Compiler.token()
  |> Compiler.parse()
  |> Compiler.compile()

  def render(name), do: unquote(@ast)
end

View.render("Elixir")
# <h1 id='title'>Elixir</h1>
```
???

I want the AST I've generated to be the body of the function. How do I do
that?

It turns out that `def` is not a keyword, but instead a macro. As a macro I
can use `unquote` to inject the AST back into the code as an expression at
compile time.

So to build the function a template is tokenized, parsed, compiled to AST and
then unquoted into the block of a function.

And now I have the beginnings of a HTML templating library that I can use in a
real app.

---
class: middle, small
```plain
ul
  for user &lt;- users
    li = user.name
```
```plain
case current_user
  match %{ role: :admin }
    p You're an admin

  match %{ role: :user }
    p You're a registered user

  match _
    p You're a guest
```
```plain
if current_user
  a(href="/log-out") Sign out
else
  a(href="/sign-in") Log in
```
???

It's missing a few things though.

I'd want to add a looping construct so I can iterate over collections, and
conditional expressions so I have have more dynamic templates. I would add
these by adapting my tokenizer and parser to output a new type of node for each
construct, and then I can build a suitable Elixir AST to get the behaviour I
want for each one.

It was about at this point that I realised something. With relatively little
effort I had created a program that takes some source code, parses it, and
then generates some code executed by the Erlang virtual machine. The output of
my program is a set of runnable functions.

Without knowing anything about compilers I've effectively written a compiler
for a micro language on the BEAM.

Getting this far was easy thanks to the excellent tools the Erlang ecosystem
has to offer. I thought presumably it wouldn't be much harder to create an
entirely new language using the same tools. As a language nerd this idea
really excited me, so I jumped right in.

---
class: middle
```ruby
module clauses

public speak {
  def (1) { "one" }
  def (2) { "two" }
  def (3) { "three" }
  def (_) { "eh?" }
}
```
???

Here's my language. It's called Gleam.

I believe that in order for a language to be worthwhile it needs to have a
clear idea of the problem it's trying to solve, and the problem Gleam is
trying to solve is the problem of there not being enough curly braces in the
Erlang world. Without curly braces we'll never be able to convince people to
come over from Javascript.

The first step in solving this problem is to make a tokenizer with Leex.

---
class: middle
```plain
Definitions.

Float   = [0-9]+\.[0-9]+
Int     = [0-9]+
String  = "([^\\""]|\\.)*"
Ident   = [a-z_][a-zA-Z0-9!\?_]*
Atom    = \:[a-zA-Z0-9!\?_-]+
WS      = [\n\s\r\t]
```
???

I've only a few definitions.

- A float and an int for numbers,
- a string,
- an identifier,
- an atom,
- an atom with quotes,
- and lastly whitespace.

---
class: middle, small
```plain
module     : {token, {module,     TokenChars}}.
private    : {token, {private,    TokenChars}}.
public     : {token, {public,     TokenChars}}.
def        : {token, {def,        TokenChars}}.
\(         : {token, {'(',        TokenChars}}.
\)         : {token, {')',        TokenChars}}.
\{         : {token, {'{',        TokenChars}}.
\}         : {token, {'}',        TokenChars}}.
\[         : {token, {'[',        TokenChars}}.
\]         : {token, {']',        TokenChars}}.
\.         : {token, {'.',        TokenChars}}.
\,         : {token, {',',        TokenChars}}.
\=         : {token, {'=',        TokenChars}}.
```
???

The first parser rules are the keywords, delimeters, and punctuation tokens,
which are straightforward.

---
class: middle, small
```plain
Rules.

{Int}    : {token, {number,     int(TokenChars)}}.
{Float}  : {token, {number,     flt(TokenChars)}}.
{String} : {token, {string,     strValue(TokenChars)}}.
{Ident}  : {token, {identifier, list_to_atom(TokenChars)}}.
{Atom}   : {token, {atom,       atomValue(TokenChars)}}.
{WS}     : skip_token.
```
???

After that comes the more complex tokens that use patterns from the
`Definitions` section. The value of each of these tokens comes from calling a
helper function on the matched characters. The `int` function converts the
value to an an integer, the `flt` function converts to a float, and for
identifier and atom I convert the value to an atom. For strings I get the
contents of the string by dropping the quotes as before. These functions are
all defined in the Erlang Code section.

Also note how the `Int` pattern and the `Float` patterns are used to build a
token of type `number`. Leex allows multiple rules to construct the same
token, so we can have variations like this.

---
class: middle, large
```plain
{WS}     : skip_token.
```
???

Lastly there's the rule for the `whitespace` definition pattern. Instead of
constructing a token the `skip_token` atom is used to signify that text
matching this pattern is to be discarded. Whitespace has no syntactic meaning
in Gleam, so it can be safely ignored.

That's the basic tokenizer done. Later I'll probably want to extend it with
mathematical operators and a pipe operator and such, but for now I can move
onto the Yecc parser.

---
class: middle, small
```ruby
module stack

public new {
  def () { [] }
}
public push {
  def (stack, item) { list.prepend(stack, item) }
}
public pop {
  def ([])    { (:error, :empty_stack) }
  def (stack) { (:ok, hd(stack), tl(stack)) }
}
```
```plain
module -> statements : '$1'.
statements -> statement            : ['$1'].
statements -> statement statements : ['$1'|'$2'].
statement -> module_declaration : '$1'.
statement -> function           : '$1'.
```
???

Here's a Gleam module called "stack". It's made up of multiple statements:

- the module declaration at the top
- and each of the function definitions are statements.

The rootsymbol of the grammar is a module. A module is defined as a series of
statements, and statements are defined as one more more statement.

A statement is either a module_declaration, or a function.

---
class: middle
```plain
% module stack
module_declaration -> module identifier
  : #module_declaration{ name = element(2, '$2') }.
```
```plain
-record(module_declaration, { name = {} }).
```
???

Module declarations are simple. They are always the `module` keyword, followed
by an identifier. Both of these are tokens, making them terminal symbols, and
leaf nodes of the AST.

To make working with nodes a little easier I've use Erlang records for each
one. The module_declaration record has a name field, in which I place the
value of the identifier.

---
class: middle, small
```ruby
public peak {
  def ([])    { (:error, :empty_stack) }
  def (stack) { (:ok, hd(stack)) }
}
```
```plain
function -> public identifier fn_block
  : #function
    { publicity = public
    , name = element(2, '$2')
    , clauses = '$3'
    }.
function -> private identifier fn_block
  : #function
    { publicity = private
    , name = element(2, '$2')
    , clauses = '$3'#fn_block.clauses
    }.
```
???

Moving on to the other type of statement-
A function is either the public or private keyword, followed by an identifier
and a function block.

The resulting function record contains

- the function publicity,
- the function name,
- and the clauses, which are taken from the the function block.

---
class: middle
```plain
fn_block -> '{' fn_statements '}' : '$2'.

fn_statements -> fn_clause
  : #fn_block
    { clauses = ['$1']
    }.
fn_statements -> fn_clause fn_statements
  : #fn_block
    { clauses = ['$1'|'$2'#fn_block.clauses]
    }.
```
???

A function block is a pair of curly braces around one or more function clauses.

Later I imagine there would also be other function block contents, such as
docstrings or unit tests.

This is why there is a record for the block, it will be easier to extend later.

---
class: middle
```ruby
def (stack) { (:ok, hd(stack)) }
```
```plain
fn_clause -> def tuple clause_block
  : #fn_clause
    { arity = length('$2')
    , arguments = '$2'
    , body = '$3'
    }.
```
???

Next the function clause.

It's the `def` keyword followed by an arguments tuple and a clause block.

Again I'm constructing a record, this one has fields for arity, arguments, and
the body.

And this continues for each parser symbol until I've defined a rule every node
in the syntax tree.

---
class: middle
```plain
module             -> #module{}
module_declaration -> #module_declaration{}
funtion            -> #function{}
fn_clause          -> #fn_clause{}
assignment         -> #assignment{}
variable           -> #variable{}
number             -> #number{}
string             -> #string{}
tuple              -> #tuple{}
list               -> #list{}
atom               -> #atom{}
call               -> #call{}
```
???

There's all the nodes that make up the Gleam AST. Each one is a record. Not
only does this make it easier to extract values from them, it also provides a
method of pattern matching on each node, which will come in handy later.

The next part of this simple compiler is to convert the Gleam abstract syntax
tree into a format and can be readily fed into the virtual machine. With the
templating language this format was Elixir AST, which would work again here.

However there are also other options. Since we've come all this way using
just the OTP standard library let's explore one of the alternatives, Core
Erlang.

---
class: middle, large
```plain
Erlang
  -> Core Erlang
  -> BEAM bytecode
```
???

Core Erlang is an intermediary language used by the Erlang compiler.

When regular Erlang code is compiled it is converted into Core Erlang code,
before being optimised and converted into the bytecode that the virtual
machine actually runs. If I target Core Erlang rather than trying to generate
bytecode directly Gleam will benefit from all the same compiler optimisations
as Erlang and hopefully be as fast.


---
class: middle, small
```plain
f(X) ->
  case X of
    {foo, A} -> B = g(A);
    {bar, A} -> B = h(A)
  end,
  {A, B}.
```
```plain
'f'/1 = fun (X) ->
  let <X1, X2> =
    case X of
      {foo, A} when 'true' ->
        let B = apply 'g'/1(A)
        in <A, B>
      {bar, A} when 'true' ->
        let B = apply 'h'/1(A)
        in <A, B>
    end,
  in {X1, X2}
```
???

Core Erlang has a textual representation which can be seen here. These code
snippets are equivilent. The first is Erlang, the second is Core Erlang, which
as you can see is much more verbose and explicit.

In addition to the textual form it also has an abstract syntax tree consisting
of regular Erlang data structures, primarily records. Great- if it's just
regular Erlang data I can construct it manually, right? Not quite.

The official documentation states that the Core Erlang AST is subject to
change without notice. It may be that when OTP 21 arrives the Core Erlang tree
has a completely difffernt format, and I'll have to re-write this section of
the compiler from scratch.


---
class: middle, large
```plain
cerl:c_atom(ok).
% => Core Erlang atom 'ok'
```
???
<!-- show data structure on slide.
     Explain this is not to be replied upon.
  -->

If the AST format is not formally specified how can it be generated? The
answer is to use the `cerl` Erlang module, which exposes functions for the
composing and decomposing of this AST.

<!-- maybe remove -->
The decomposition functions could possibly be useful for reflection in a
fashion similar to how the Elixir linter worked, but for the job at hand I'm
interested in the functions for constructing AST.

Here we can see the `c_atom` constructor being used.
It is a function takes an atom and returns the Core Erlang node that
represents an atom. What exactly that looks like doesn't matter, as it may
change in a later OTP version. The only thing that matters is that we trust
that this function will return the correct node data structure, whatever that
may be.


---
class: middle
```plain
c_alias/2       c_cons_skel/2        c_primop/2
c_apply/2       c_float/1            c_receive/1
c_atom/1        c_fname/2            c_receive/3
c_binary/1      c_fun/2              c_seq/2
c_bitstr/3      c_int/1              c_string/1
c_bitstr/4      c_let/3              c_try/5
c_bitstr/5      c_letrec/2           c_tuple/1
c_call/3        c_map/1              c_tuple_skel/1
c_case/2        c_map_pair/2         c_values/1
c_catch/1       c_map_pair_exact/2   c_var/1
c_char/1        c_map_pattern/1
c_clause/2      c_module/3
c_clause/3      c_module/4
c_cons/2        c_nil/0
```
???

There are similar functions for all the other nodes. With these I can convert
the Gleam AST to the Core Erlang AST by traversing the tree and calling the
appropriate constructor for that node.

This is where the Gleam parser's use of records comes in handy- I can create a
function that takes a Gleam node and returns Core Erlang by defining a
function clause for each record.

---
class: middle
```plain
codegen(Node = #string{}) ->
  cerl:c_string(Node#string.value).
```
???

For example, here's the clause for the string record, and thus the string
node. It just calls the `c_string` constructor on the value of the string
record.

---
class: middle
```plain
codegen(#string{ value = Value }) ->
  cerl:c_string(Value);

codegen(#number{ value = Value }) when is_integer(Value) ->
  cerl:c_integer(Value);

codegen(#number{ value = Value }) when is_float(Value) ->
  cerl:c_float(Value).
```
???

Here's the clauses for numbers. The Gleam AST doesn't match up perfectly with
the Core Erlang one, so I've used a guard to differentiate between ints and
floats. After that it's just the matter of calling the correct constructor for
each type.

All the nodes seen so far have been leaf nodes, so they've been simple to
convert to Core Erlang. Branch nodes are a little more complex.

Branch nodes are nodes that have children, they consist of multiple nodes
joined into one. Examples would be a list or a tuple that contains multiple
elements, or a function which has a name and multiple clauses.

Converting these nodes is more more complex as not only do they themselves
need to be converted to Core Erlang, but their children need to be converted
as well.

---
class: middle
```ruby
(:ok, "hello")
```
```plain
#tuple
{ elements = [ #atom{ value = ok }
             , #string{ value = "Hello" }
             ]
}
```
```plain
codegen(#tuple{ elements = Elements }) ->
  Cerls = lists:map(fun codegen/1, Elements),
  cerl:c_tuple(Cerls).
```
???

Here is a tuple node. It's children are the elements of the tuple.
This tuple contains the atom "ok" and the string "hello", so the children
would be would be the atom node and the string node.

In the Gleam AST this tuple node is a record with an elements property, which
is a list containing the atom and the string.

The clause that handles converting tuples first maps the codegen
function over the elements list to convert the contents to Core Erlang. Once
the children are converted the `c_tuple` function is called on the new
elements to create a Core Erlang tuple.

Here the children are leaf nodes, but what if the children were also branch
nodes? For example, what if the tuple contained another tuple, and that tuple
contained a list, and on?

Each clause that handles a branch node calls the codegen function on
each of its children. Because of this when the codegen function is called on
the root node of the tree, the function is recursively called on the modules
children, and their children, and so on until leaf nodes are reached, and the
entire tree has been converted.

---
class: middle
```plain
#module
{ name = my_module
, functions = [ #function{}, #function{}, #function{} ]
}
```
```plain
codegen(#module{ name = Name, functions = Functions }) ->
  CerlName = cerl:c_atom(Name),
  CerlExports = gen_exports(Functions),
  CerlFuncs = lists:map(fun codegen/1, Functions),
  cerl:c_module(CerlName, CerlExports, CerlFuncs).
```
???

Jumping from the bottom of the tree to the top, here is the module record.
It's quite considerably more complex than the previous nodes.

The cerl constructor takes 3 arguments. The module name, a list of functions
to export, and a list of the module functions themselves. And of course, each
one of these needs to be given in their Core Erlang form.

The name is easy- it's just an atom, so `c_atom` is called on the name.

For forming the list of exports I've created a `gen_exports` function. This
filters the list of functions for those that are public, and then
constructs a Core Erlang export for each one.

Lastly the functions are transformed into Core Erlang by mapping the `codegen`
function over them, which in turn recursively transforms their children, and
their children, recursively converting the tree into Core Erlang.


---
class: middle, small
```plain
%% @spec c_module(Name::cerl(), Exports, Definitions) -> cerl()
%%
%%     Exports = [cerl()]
%%     Definitions = [{cerl(), cerl()}]
%%
%% @equiv c_module(Name, Exports, [], Definitions)

-spec c_module(cerl(), [cerl()], [{cerl(), cerl()}]) -> c_module().

c_module(Name, Exports, Es) ->
    % ...
```
???

I found that with more complex nodes it's not immediately obvious how to use
the constructor function. If you're going to play with `cerl` I recommend
downloading a copy of the OTP source code and reading the documentation in the
module itself, and also paying close attention to the type specifications
which can be very informative.

When I still had problems I found a good trick was to turn to the source code
of a BEAM language that uses this module, and see how they use it.

I found LFE, MLFE, and Joxa were good language to study.

---
class: middle, small
```plain
-module(gleam).
-export([load_file]).

load_file(Path, ModuleName) ->
  {ok, Binary} = file:read_file(Path),
  Source = unicode:characters_to_list(Binary).
  {ok, Tokens, _} = gleam_tokenizer:string(Source),
  {ok, AST} = gleam_parser:parse(Tokens),
  {ok, Cerl} = gleam_codegen:codegen(AST),
  {ok, _, Beam} = compile:forms(Cerl, [report, verbose, from_core]),
  {module, Name} = code:load_binary(Name, Path, Beam),
  ok.
```
???

Right. With a codegen function clause defined for each node in the AST I've
all the basic parts of a BEAM language compiler. The only this that is left is
to stick them all together.

Here is the `gleam_compiler` module, which defines a `load_file` function.

This function takes a path to a file of Gleam source code, reads the file and
converts the resulting binary into an Erlang string.

The string is then fed into the tokenizer, which breaks the source code down
into a list of it's atomic parts, such as strings, numbers, and punctuation.

The tokens are fed into the parser which constructs an abstract syntax tree
from them. This tree contains the syntactic information of the code. Lists
contain elements, functions have clauses, and so on.

This tree is then fed into the codegen function which converts it into a
format that can be loaded into the BEAM. Here that format is the Core Erlang,
the intermediary language that Erlang compiles to.

The Core Erlang is then compiled into BEAM bytecode using Erlang's
`compile:forms` function, and then loaded into the virtual machine using
`code:load_binary`.

---
class: middle
```ruby
# src/first_module.gleam

module first_module

public hello {
  def () { "Hello, world!" }
}
```
```plain
gleam:load_file("src/first_module.gleam", first_module).
% => ok

first_module:hello().
% => "Hello, world!"
```
???

Here's a Gleam module. If I call the `load_file` function on it the file will
be read, tokenized, parsed, transformed into Core Erlang, and then loaded into
the BEAM.

Then the Glean function can be called, just like any other Erlang function.
And with that there's a new language running on the BEAM!

I was ecstatic when I successfully compiled my first module. It's such a small
thing, but it was a really fun journey, and a unexpectedly smooth one too.

---
class: middle, large

|                 |                         |
| --------------- | -------------           |
| Tokenization    | Leex                    |
| Parsing         | Yecc                    |
| Code generation | Elixir AST, Core Erlang |

???

With Erlang and Elixir we are supplied with a range of excellent tools
that can be used by themselves, or all together.

Leex and Yecc give us an easy out-of-the-box way of doing tokenization and
parsing of code, and with Elixir macros and Core Erlang we have an friendly
way to generate code that can be loaded into the virtual machine. Coupled with
Elixir and Erlang's excellent pattern matching and data handling these tasks
become quite straightforward.

---
class: middle, large

- Code formatter
- Security analysis
- Style linters
- Parsers
- Type checker
- More languages

???

I think it would be really exciting to see more projects making use of what
ecosystem gives us here.

I'd love to see more static analysis tools, and a code formatter in the style
of gofmt or elmformat. It could also be interesting to a language with a
powerful ML style type system on the BEAM. Some work has been done here with
the MLFE project, it'd be great to see it develop into something.

I've really enjoyed working in this space. If you think you could also find it
fun I encourage you to go and build your own Elixir. There's plenty of space
on the BEAM for exciting new projects.

Thank you very much.
---
class: center, middle, large, inverse

Thank you everyone who has worked on Erlang, Elixir, LFE, MLFE, Joxa, Rubocop,
Dogma, Slim, JSCS, and EEx.

Louis Pilfold<br>
[@louispilfold](https://twitter.com/louispilfold)<br>
[github.com/lpil](https://github.com/lpil)
    </textarea>
    <script src="../vendor/remark-0.11.0.min.js"></script>
    <!-- <script src="../vendor/highlight-js/elixir.min.js"></script> -->
    <!-- <script src="../vendor/highlight-js/erlang.min.js"></script> -->
    <script>
var slideshow = remark.create({
  highlightStyle: "monokai",
  highlightLanguage: "remark",
  highlightLines: true,
});
    </script>
  </body>
</html>
